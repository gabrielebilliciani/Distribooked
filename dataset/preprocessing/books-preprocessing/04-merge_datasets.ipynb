{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac16346-b234-4e7f-89f2-492b1cc8b23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gutenberg dataset...\n",
      "Gutenberg dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 74594 entries, 0 to 74593\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   title        74594 non-null  object\n",
      " 1   authors      74594 non-null  object\n",
      " 2   issued       74594 non-null  object\n",
      " 3   language     74594 non-null  object\n",
      " 4   subjects     74594 non-null  object\n",
      " 5   bookshelves  74594 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 3.4+ MB\n",
      "None\n",
      "\n",
      "Gutenberg dataset columns: ['title', 'authors', 'issued', 'language', 'subjects', 'bookshelves']\n",
      "\n",
      "Number of records in Gutenberg dataset: 74594\n",
      "\n",
      "Loading Amazon dataset chunks...\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0000.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Literature & Fiction', 'History & Criticism']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0001.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Mystery, Thriller & Suspense', 'Mystery']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0002.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Cookbooks, Food & Wine', 'Cooking Education & Reference']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0003.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Teen & Young Adult', 'Literature & Fiction']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0004.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Law', 'Rules & Procedures']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0005.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Engineering & Transportation', 'Transportation']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0006.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Literature & Fiction', 'Poetry']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0007.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', \"Children's Books\", 'History']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0008.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: []\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0009.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Science Fiction & Fantasy', 'Fantasy']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0010.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Literature & Fiction', 'Genre Fiction']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0011.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Law', 'Business']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0012.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Literature & Fiction', 'Erotica']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0013.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'New, Used & Rental Textbooks', 'Humanities']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0014.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', \"Children's Books\", 'Arts, Music & Photography']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0015.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Teen & Young Adult', 'Literature & Fiction']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0016.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Engineering & Transportation', 'Engineering']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0017.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'New, Used & Rental Textbooks', 'Medicine & Health Sciences']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0018.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Business & Money', 'Business Development & Entrepreneurship']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0019.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Christian Books & Bibles', 'Churches & Church Leadership']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0020.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', \"Children's Books\", 'Growing Up & Facts of Life']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0021.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Medical Books', 'Medicine']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0022.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', \"Children's Books\", 'Literature & Fiction']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0023.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'History', 'Military']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0024.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Arts & Photography', 'Photography & Video']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0025.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', \"Children's Books\"]\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0026.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Literature & Fiction', 'Genre Fiction']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0027.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Self-Help', 'Motivational']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0028.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Education & Teaching', 'Schools & Teaching']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0029.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: []\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0030.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Christian Books & Bibles', 'Literature & Fiction']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0031.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Arts & Photography', 'Architecture']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0032.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Christian Books & Bibles', 'Bible Study & Reference']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0033.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Mystery, Thriller & Suspense', 'Thrillers & Suspense']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0034.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: []\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0035.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Christian Books & Bibles', 'Literature & Fiction']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0036.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'History', 'World']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0037.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Arts & Photography', 'Performing Arts']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0038.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Literature & Fiction', 'Genre Fiction']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0039.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Business & Money', 'Management & Leadership']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0040.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Arts & Photography', 'Decorative Arts & Design']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0041.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Foreign Language Books', 'Japanese']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0042.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: []\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0043.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Literature & Fiction', 'Poetry']\n",
      "\n",
      "Processing chunk file: preprocessed_chunks\\amazon_preprocessed_chunk_0044.json\n",
      "Sample book keys: dict_keys(['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'])\n",
      "Sample categories: ['Books', 'Business & Money', 'Economics']\n",
      "\n",
      "Amazon dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2381122 entries, 0 to 2381121\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Dtype \n",
      "---  ------          ----- \n",
      " 0   title           object\n",
      " 1   subtitle        object\n",
      " 2   author_name     object\n",
      " 3   author_avatar   object\n",
      " 4   author_about    object\n",
      " 5   categories      object\n",
      " 6   publisher       object\n",
      " 7   ISBN 10         object\n",
      " 8   ISBN 13         object\n",
      " 9   main_image_url  object\n",
      "dtypes: object(10)\n",
      "memory usage: 181.7+ MB\n",
      "None\n",
      "\n",
      "Amazon dataset columns: ['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url']\n",
      "\n",
      "Number of records in Amazon dataset: 2381122\n",
      "\n",
      "Sample of categories from Amazon dataset:\n",
      "0    [Books, Literature & Fiction, History & Critic...\n",
      "1        [Books, Reference, Words, Language & Grammar]\n",
      "2    [Books, Biographies & Memoirs, Leaders & Notab...\n",
      "3    [Books, Engineering & Transportation, Engineer...\n",
      "4    [Books, Education & Teaching, Schools & Teaching]\n",
      "Name: categories, dtype: object\n",
      "\n",
      "Number of books with categories: 2381122\n",
      "\n",
      "Merging datasets...\n",
      "\n",
      "Pre-merge verification:\n",
      "Gutenberg dataset shape: (74594, 6)\n",
      "Amazon dataset shape: (2381122, 10)\n",
      "\n",
      "Amazon dataset columns before merge: ['title', 'subtitle', 'author_name', 'author_avatar', 'author_about', 'categories', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url']\n",
      "\n",
      "Sample of cleaned titles from both datasets:\n",
      "\n",
      "Gutenberg cleaned titles:\n",
      "                                               title  \\\n",
      "0  The Declaration of Independence of the United ...   \n",
      "1  The United States Bill of Rights\\r\\nThe Ten Or...   \n",
      "2                John F. Kennedy's Inaugural Address   \n",
      "3  Lincoln's Gettysburg Address\\r\\nGiven November...   \n",
      "4                     The United States Constitution   \n",
      "\n",
      "                                       cleaned_title  \n",
      "0  the declaration of independence of the united ...  \n",
      "1  the united states bill of rights the ten origi...  \n",
      "2                 john f kennedy s inaugural address  \n",
      "3  lincoln s gettysburg address given november 19...  \n",
      "4                     the united states constitution  \n",
      "\n",
      "Amazon cleaned titles:\n",
      "                                           title  \\\n",
      "0                                        Chaucer   \n",
      "1                        Notes from a Kidwatcher   \n",
      "2                    Service: A Navy SEAL at War   \n",
      "3  Make: Electronics: Learning Through Discovery   \n",
      "4           Four Centuries of American Education   \n",
      "\n",
      "                                 cleaned_title  \n",
      "0                                      chaucer  \n",
      "1                      notes from a kidwatcher  \n",
      "2                   service a navy seal at war  \n",
      "3  make electronics learning through discovery  \n",
      "4         four centuries of american education  \n",
      "\n",
      "Verifying Amazon columns before merge:\n",
      "Column 'cleaned_title' exists in Amazon dataset\n",
      "Column 'subtitle' exists in Amazon dataset\n",
      "Column 'publisher' exists in Amazon dataset\n",
      "Column 'ISBN 10' exists in Amazon dataset\n",
      "Column 'ISBN 13' exists in Amazon dataset\n",
      "Column 'main_image_url' exists in Amazon dataset\n",
      "Column 'categories' exists in Amazon dataset\n",
      "\n",
      "Post-merge verification:\n",
      "Merged dataset shape: (100866, 13)\n",
      "\n",
      "Merged dataset columns: ['title', 'authors', 'issued', 'language', 'subjects', 'bookshelves', 'cleaned_title', 'subtitle', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url', 'categories']\n",
      "\n",
      "Categories in merged dataset:\n",
      "Number of books with categories: 33909\n",
      "\n",
      "Sample of books with categories:\n",
      "\n",
      "Title: Give Me Liberty or Give Me Death\n",
      "Categories: ['Books', 'Politics & Social Sciences', 'Politics & Government']\n",
      "\n",
      "Title: Alice's Adventures in Wonderland\n",
      "Categories: ['Books', \"Children's Books\", 'Science Fiction & Fantasy']\n",
      "\n",
      "Title: Alice's Adventures in Wonderland\n",
      "Categories: ['Books', \"Children's Books\", 'Classics']\n",
      "\n",
      "Final dataset verification:\n",
      "\n",
      "Columns in final dataset: ['title', 'authors', 'issued', 'language', 'subjects', 'bookshelves', 'subtitle', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url', 'categories']\n",
      "\n",
      "Sample of categories in final dataset:\n",
      "\n",
      "Title: Give Me Liberty or Give Me Death\n",
      "Categories: ['Books', 'Politics & Social Sciences', 'Politics & Government']\n",
      "\n",
      "Title: Alice's Adventures in Wonderland\n",
      "Categories: ['Books', \"Children's Books\", 'Science Fiction & Fantasy']\n",
      "\n",
      "Title: Alice's Adventures in Wonderland\n",
      "Categories: ['Books', \"Children's Books\", 'Classics']\n",
      "\n",
      "Saving merged dataset...\n",
      "\n",
      "Verifying saved file...\n",
      "\n",
      "Sample book from saved file:\n",
      "Available fields: ['title', 'authors', 'issued', 'language', 'subjects', 'bookshelves', 'subtitle', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url', 'categories']\n",
      "Categories: None\n",
      "\n",
      "Merge complete! Results saved to merged_dataset.json\n",
      "Successfully matched 33909 out of 100866 books\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "def load_gutenberg_dataset(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and process the Gutenberg dataset from a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the Gutenberg dataset JSON file\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame containing the Gutenberg dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            gutenberg_data = json.load(f)\n",
    "        \n",
    "        # Convert to DataFrame and ensure it's in the right format\n",
    "        df = pd.DataFrame(gutenberg_data)\n",
    "        \n",
    "        # Print detailed information about the dataset\n",
    "        print(\"Gutenberg dataset info:\")\n",
    "        print(df.info())\n",
    "        print(\"\\nGutenberg dataset columns:\", df.columns.tolist())\n",
    "        print(f\"\\nNumber of records in Gutenberg dataset: {len(df)}\")\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Gutenberg dataset: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def load_amazon_chunks(chunks_directory: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and concatenate all Amazon dataset chunks from a directory.\n",
    "    \n",
    "    Args:\n",
    "        chunks_directory: Directory containing Amazon dataset chunk files\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame containing the combined Amazon dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get all JSON files in the directory\n",
    "        chunk_files = glob.glob(str(Path(chunks_directory) / \"amazon_preprocessed_chunk_*.json\"))\n",
    "        \n",
    "        if not chunk_files:\n",
    "            raise FileNotFoundError(f\"No Amazon chunk files found in {chunks_directory}\")\n",
    "        \n",
    "        all_chunks = []\n",
    "        for chunk_file in chunk_files:\n",
    "            with open(chunk_file, 'r', encoding='utf-8') as f:\n",
    "                chunk_data = json.load(f)\n",
    "                # Verify the structure of each chunk\n",
    "                print(f\"\\nProcessing chunk file: {chunk_file}\")\n",
    "                if isinstance(chunk_data, dict):\n",
    "                    print(f\"Keys in chunk file: {chunk_data.keys()}\")\n",
    "                    chunk_data = chunk_data.get('books', chunk_data)\n",
    "                \n",
    "                # Verify categories exist in the chunk data\n",
    "                if isinstance(chunk_data, list) and chunk_data:\n",
    "                    sample_book = chunk_data[0]\n",
    "                    print(f\"Sample book keys: {sample_book.keys()}\")\n",
    "                    if 'categories' in sample_book:\n",
    "                        print(f\"Sample categories: {sample_book['categories']}\")\n",
    "                \n",
    "                all_chunks.extend(chunk_data if isinstance(chunk_data, list) else [chunk_data])\n",
    "        \n",
    "        df = pd.DataFrame(all_chunks)\n",
    "        \n",
    "        # Print detailed information about the Amazon dataset\n",
    "        print(\"\\nAmazon dataset info:\")\n",
    "        print(df.info())\n",
    "        print(\"\\nAmazon dataset columns:\", df.columns.tolist())\n",
    "        print(f\"\\nNumber of records in Amazon dataset: {len(df)}\")\n",
    "        \n",
    "        # Verify categories column\n",
    "        if 'categories' in df.columns:\n",
    "            print(\"\\nSample of categories from Amazon dataset:\")\n",
    "            print(df['categories'].head())\n",
    "            print(f\"\\nNumber of books with categories: {df['categories'].notna().sum()}\")\n",
    "        else:\n",
    "            print(\"\\nWARNING: 'categories' column not found in Amazon dataset!\")\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Amazon chunks: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def merge_datasets(gutenberg_df: pd.DataFrame, amazon_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge Gutenberg and Amazon datasets based on matching titles, including categories.\n",
    "    \n",
    "    Args:\n",
    "        gutenberg_df: DataFrame containing Gutenberg dataset\n",
    "        amazon_df: DataFrame containing Amazon dataset\n",
    "    \n",
    "    Returns:\n",
    "        Merged DataFrame with additional Amazon fields including categories\n",
    "    \"\"\"\n",
    "    # Verify input data\n",
    "    print(\"\\nPre-merge verification:\")\n",
    "    print(f\"Gutenberg dataset shape: {gutenberg_df.shape}\")\n",
    "    print(f\"Amazon dataset shape: {amazon_df.shape}\")\n",
    "    print(\"\\nAmazon dataset columns before merge:\", amazon_df.columns.tolist())\n",
    "    \n",
    "    # Clean titles in both datasets\n",
    "    gutenberg_df = clean_titles(gutenberg_df)\n",
    "    amazon_df = clean_titles(amazon_df)\n",
    "    \n",
    "    # Verify cleaned titles\n",
    "    print(\"\\nSample of cleaned titles from both datasets:\")\n",
    "    print(\"\\nGutenberg cleaned titles:\")\n",
    "    print(gutenberg_df[['title', 'cleaned_title']].head())\n",
    "    print(\"\\nAmazon cleaned titles:\")\n",
    "    print(amazon_df[['title', 'cleaned_title']].head())\n",
    "    \n",
    "    # Store the columns we want to merge\n",
    "    amazon_columns = ['cleaned_title', 'subtitle', 'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url', 'categories']\n",
    "    print(\"\\nVerifying Amazon columns before merge:\")\n",
    "    for col in amazon_columns:\n",
    "        if col in amazon_df.columns:\n",
    "            print(f\"Column '{col}' exists in Amazon dataset\")\n",
    "        else:\n",
    "            print(f\"WARNING: Column '{col}' missing from Amazon dataset!\")\n",
    "    \n",
    "    # Perform the merge\n",
    "    merged_df = pd.merge(\n",
    "        gutenberg_df,\n",
    "        amazon_df[amazon_columns],\n",
    "        on='cleaned_title',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Verify merge results\n",
    "    print(\"\\nPost-merge verification:\")\n",
    "    print(f\"Merged dataset shape: {merged_df.shape}\")\n",
    "    print(\"\\nMerged dataset columns:\", merged_df.columns.tolist())\n",
    "    \n",
    "    # Check categories specifically\n",
    "    if 'categories' in merged_df.columns:\n",
    "        print(\"\\nCategories in merged dataset:\")\n",
    "        print(f\"Number of books with categories: {merged_df['categories'].notna().sum()}\")\n",
    "        print(\"\\nSample of books with categories:\")\n",
    "        sample = merged_df[merged_df['categories'].notna()].head(3)\n",
    "        for _, row in sample.iterrows():\n",
    "            print(f\"\\nTitle: {row['title']}\")\n",
    "            print(f\"Categories: {row['categories']}\")\n",
    "    else:\n",
    "        print(\"\\nWARNING: 'categories' column not present in merged dataset!\")\n",
    "    \n",
    "    # Remove the cleaning column\n",
    "    merged_df = merged_df.drop('cleaned_title', axis=1)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def clean_titles(df: pd.DataFrame, title_column: str = 'title') -> pd.DataFrame:\n",
    "    \"\"\"[Previous implementation remains the same]\"\"\"\n",
    "    df = df.copy()\n",
    "    if title_column not in df.columns:\n",
    "        available_columns = df.columns.tolist()\n",
    "        raise KeyError(f\"Column '{title_column}' not found. Available columns are: {available_columns}\")\n",
    "    \n",
    "    df['cleaned_title'] = df[title_column].str.lower()\n",
    "    df['cleaned_title'] = (df['cleaned_title']\n",
    "                          .str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
    "                          .str.replace(r'\\s+', ' ', regex=True)\n",
    "                          .str.strip())\n",
    "    return df\n",
    "\n",
    "def main(gutenberg_path: str, amazon_chunks_dir: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the dataset merging process.\n",
    "    \n",
    "    Args:\n",
    "        gutenberg_path: Path to the Gutenberg dataset JSON file\n",
    "        amazon_chunks_dir: Directory containing Amazon dataset chunks\n",
    "        output_path: Path where the merged dataset should be saved\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    print(\"Loading Gutenberg dataset...\")\n",
    "    gutenberg_df = load_gutenberg_dataset(gutenberg_path)\n",
    "    \n",
    "    print(\"\\nLoading Amazon dataset chunks...\")\n",
    "    amazon_df = load_amazon_chunks(amazon_chunks_dir)\n",
    "    \n",
    "    # Perform the merge\n",
    "    print(\"\\nMerging datasets...\")\n",
    "    merged_df = merge_datasets(gutenberg_df, amazon_df)\n",
    "    \n",
    "    # Verify final dataset before saving\n",
    "    print(\"\\nFinal dataset verification:\")\n",
    "    print(\"\\nColumns in final dataset:\", merged_df.columns.tolist())\n",
    "    if 'categories' in merged_df.columns:\n",
    "        categories_sample = merged_df[merged_df['categories'].notna()].head(3)\n",
    "        print(\"\\nSample of categories in final dataset:\")\n",
    "        for _, row in categories_sample.iterrows():\n",
    "            print(f\"\\nTitle: {row['title']}\")\n",
    "            print(f\"Categories: {row['categories']}\")\n",
    "    \n",
    "    # Save the result\n",
    "    print(\"\\nSaving merged dataset...\")\n",
    "    merged_df.to_json(output_path, orient='records', indent=4)\n",
    "    \n",
    "    # Verify saved file\n",
    "    print(\"\\nVerifying saved file...\")\n",
    "    try:\n",
    "        with open(output_path, 'r', encoding='utf-8') as f:\n",
    "            saved_data = json.load(f)\n",
    "        sample_book = saved_data[0]\n",
    "        print(\"\\nSample book from saved file:\")\n",
    "        print(f\"Available fields: {list(sample_book.keys())}\")\n",
    "        if 'categories' in sample_book:\n",
    "            print(f\"Categories: {sample_book.get('categories')}\")\n",
    "        else:\n",
    "            print(\"WARNING: 'categories' not found in saved data!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error verifying saved file: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nMerge complete! Results saved to {output_path}\")\n",
    "    print(f\"Successfully matched {merged_df['ISBN 10'].notna().sum()} out of {len(merged_df)} books\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\n",
    "        gutenberg_path=\"datasets/preprocessed_gutenberg.json\",\n",
    "        amazon_chunks_dir=\"preprocessed_chunks\",\n",
    "        output_path=\"merged_dataset.json\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f821fc-b890-4fd1-b167-c72b77894c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
