{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1318437-29a1-401e-8d41-95fce6a0b143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk_0000.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (90714, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (60236, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0000.json...\n",
      "Successfully processed chunk_0000.jsonl\n",
      "Processing chunk_0001.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91200, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (58228, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0001.json...\n",
      "Successfully processed chunk_0001.jsonl\n",
      "Processing chunk_0002.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91364, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (57205, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0002.json...\n",
      "Successfully processed chunk_0002.jsonl\n",
      "Processing chunk_0003.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91512, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (57431, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0003.json...\n",
      "Successfully processed chunk_0003.jsonl\n",
      "Processing chunk_0004.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91293, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (57563, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0004.json...\n",
      "Successfully processed chunk_0004.jsonl\n",
      "Processing chunk_0005.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91125, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (57385, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0005.json...\n",
      "Successfully processed chunk_0005.jsonl\n",
      "Processing chunk_0006.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91464, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (57684, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0006.json...\n",
      "Successfully processed chunk_0006.jsonl\n",
      "Processing chunk_0007.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91538, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (57529, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0007.json...\n",
      "Successfully processed chunk_0007.jsonl\n",
      "Processing chunk_0008.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91478, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (57360, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0008.json...\n",
      "Successfully processed chunk_0008.jsonl\n",
      "Processing chunk_0009.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91374, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (57055, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0009.json...\n",
      "Successfully processed chunk_0009.jsonl\n",
      "Processing chunk_0010.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91432, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (57476, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0010.json...\n",
      "Successfully processed chunk_0010.jsonl\n",
      "Processing chunk_0011.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91482, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (57603, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0011.json...\n",
      "Successfully processed chunk_0011.jsonl\n",
      "Processing chunk_0012.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91301, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (57424, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0012.json...\n",
      "Successfully processed chunk_0012.jsonl\n",
      "Processing chunk_0013.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91256, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (57483, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0013.json...\n",
      "Successfully processed chunk_0013.jsonl\n",
      "Processing chunk_0014.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91262, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (56889, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0014.json...\n",
      "Successfully processed chunk_0014.jsonl\n",
      "Processing chunk_0015.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91380, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (57659, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0015.json...\n",
      "Successfully processed chunk_0015.jsonl\n",
      "Processing chunk_0016.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91345, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (56690, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0016.json...\n",
      "Successfully processed chunk_0016.jsonl\n",
      "Processing chunk_0017.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91499, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (57194, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0017.json...\n",
      "Successfully processed chunk_0017.jsonl\n",
      "Processing chunk_0018.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91531, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (56670, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0018.json...\n",
      "Successfully processed chunk_0018.jsonl\n",
      "Processing chunk_0019.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91452, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (55034, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0019.json...\n",
      "Successfully processed chunk_0019.jsonl\n",
      "Processing chunk_0020.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91398, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (51487, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0020.json...\n",
      "Successfully processed chunk_0020.jsonl\n",
      "Processing chunk_0021.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91443, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (50217, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0021.json...\n",
      "Successfully processed chunk_0021.jsonl\n",
      "Processing chunk_0022.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91387, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (54988, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0022.json...\n",
      "Successfully processed chunk_0022.jsonl\n",
      "Processing chunk_0023.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91360, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (55319, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0023.json...\n",
      "Successfully processed chunk_0023.jsonl\n",
      "Processing chunk_0024.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91186, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (53170, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0024.json...\n",
      "Successfully processed chunk_0024.jsonl\n",
      "Processing chunk_0025.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91300, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (50791, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0025.json...\n",
      "Successfully processed chunk_0025.jsonl\n",
      "Processing chunk_0026.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91217, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (53641, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0026.json...\n",
      "Successfully processed chunk_0026.jsonl\n",
      "Processing chunk_0027.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91139, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (54969, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0027.json...\n",
      "Successfully processed chunk_0027.jsonl\n",
      "Processing chunk_0028.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91304, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (55093, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0028.json...\n",
      "Successfully processed chunk_0028.jsonl\n",
      "Processing chunk_0029.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91187, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (55243, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0029.json...\n",
      "Successfully processed chunk_0029.jsonl\n",
      "Processing chunk_0030.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91327, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (55513, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0030.json...\n",
      "Successfully processed chunk_0030.jsonl\n",
      "Processing chunk_0031.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91321, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (54180, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0031.json...\n",
      "Successfully processed chunk_0031.jsonl\n",
      "Processing chunk_0032.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91325, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (54490, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0032.json...\n",
      "Successfully processed chunk_0032.jsonl\n",
      "Processing chunk_0033.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91338, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (54976, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0033.json...\n",
      "Successfully processed chunk_0033.jsonl\n",
      "Processing chunk_0034.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (91433, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (54950, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0034.json...\n",
      "Successfully processed chunk_0034.jsonl\n",
      "Processing chunk_0035.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (92505, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (47589, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0035.json...\n",
      "Successfully processed chunk_0035.jsonl\n",
      "Processing chunk_0036.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (93209, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (44577, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0036.json...\n",
      "Successfully processed chunk_0036.jsonl\n",
      "Processing chunk_0037.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (93011, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (43980, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0037.json...\n",
      "Successfully processed chunk_0037.jsonl\n",
      "Processing chunk_0038.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (92976, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (44888, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0038.json...\n",
      "Successfully processed chunk_0038.jsonl\n",
      "Processing chunk_0039.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (93092, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (44467, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0039.json...\n",
      "Successfully processed chunk_0039.jsonl\n",
      "Processing chunk_0040.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (93075, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (44288, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0040.json...\n",
      "Successfully processed chunk_0040.jsonl\n",
      "Processing chunk_0041.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (93021, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (44477, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0041.json...\n",
      "Successfully processed chunk_0041.jsonl\n",
      "Processing chunk_0042.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (93235, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (44269, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0042.json...\n",
      "Successfully processed chunk_0042.jsonl\n",
      "Processing chunk_0043.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (100000, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (93071, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (44466, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0043.json...\n",
      "Successfully processed chunk_0043.jsonl\n",
      "Processing chunk_0044.jsonl...\n",
      "Reading JSON file...\n",
      "Initial dataframe shape: (48181, 16)\n",
      "Processing nested fields...\n",
      "Processing author field...\n",
      "Processing images field...\n",
      "Extracting ISBNs and Publisher from details...\n",
      "Filtering for Books category...\n",
      "After Books filtering: (44923, 22)\n",
      "Selecting final columns...\n",
      "Dropping rows with missing values...\n",
      "After dropping missing values: (21296, 10)\n",
      "Restoring final structure...\n",
      "Saving to preprocessed_chunks/amazon_preprocessed_chunk_0044.json...\n",
      "Successfully processed chunk_0044.jsonl\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "# Define the input and output directories\n",
    "input_dir = \"datasets/original_chunks/\"\n",
    "output_dir = \"preprocessed_chunks/\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Helper functions to safely handle problematic fields\n",
    "def safe_join(x):\n",
    "    if isinstance(x, list):\n",
    "        return \" \".join(str(item) for item in x)\n",
    "    if isinstance(x, float) and pd.isna(x):\n",
    "        return None\n",
    "    return str(x)\n",
    "\n",
    "def safe_split(x):\n",
    "    if isinstance(x, str):\n",
    "        return x.split()\n",
    "    if isinstance(x, float) and pd.isna(x):\n",
    "        return []\n",
    "    return []\n",
    "\n",
    "def extract_field(details, field_name):\n",
    "    \"\"\"Generic function to extract fields from nested details\"\"\"\n",
    "    if isinstance(details, list):\n",
    "        for item in details:\n",
    "            if isinstance(item, dict) and field_name in item:\n",
    "                return item[field_name]\n",
    "    elif isinstance(details, dict):\n",
    "        if field_name in details:\n",
    "            return details[field_name]\n",
    "    return None\n",
    "\n",
    "def safe_process_chunk(chunk_file):\n",
    "    print(f\"Processing {chunk_file}...\")\n",
    "    input_file_path = os.path.join(input_dir, chunk_file)\n",
    "    output_file_name = f\"amazon_preprocessed_{os.path.splitext(chunk_file)[0]}.json\"\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "    \n",
    "    # Step 1: Import the dataset\n",
    "    print(\"Reading JSON file...\")\n",
    "    try:\n",
    "        df = pd.read_json(input_file_path, lines=True)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"File {chunk_file} is empty, creating empty json file\")\n",
    "        result = []\n",
    "        print(f\"Saving to {output_file_path}...\")\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "        return False\n",
    "    \n",
    "    print(f\"Initial dataframe shape: {df.shape}\")\n",
    "    \n",
    "    try:\n",
    "        # Step 2: Process nested fields first\n",
    "        print(\"Processing nested fields...\")\n",
    "        # Handle author field\n",
    "        if 'author' in df.columns:\n",
    "            print(\"Processing author field...\")\n",
    "            df['author_name'] = df['author'].apply(lambda x: x.get('name') if isinstance(x, dict) else None)\n",
    "            df['author_avatar'] = df['author'].apply(lambda x: x.get('avatar') if isinstance(x, dict) else None)\n",
    "            df['author_about'] = df['author'].apply(lambda x: safe_join(x.get('about', [])) if isinstance(x, dict) else None)\n",
    "\n",
    "        # Handle images field\n",
    "        if 'images' in df.columns:\n",
    "            print(\"Processing images field...\")\n",
    "            df['main_image_url'] = df['images'].apply(\n",
    "                lambda imgs: next((img.get('large') for img in (imgs if isinstance(imgs, list) else [])\n",
    "                                    if isinstance(img, dict) and img.get('variant') == 'MAIN'), None)\n",
    "            )\n",
    "            \n",
    "        # Extract ISBNs and Publisher from 'details'\n",
    "        if 'details' in df.columns:\n",
    "            print(\"Extracting ISBNs and Publisher from details...\")\n",
    "            df['ISBN 10'] = df['details'].apply(lambda x: extract_field(x, 'ISBN-10') or extract_field(x, 'ISBN10') or extract_field(x, 'ISBN 10'))\n",
    "            df['ISBN 13'] = df['details'].apply(lambda x: extract_field(x, 'ISBN-13') or extract_field(x, 'ISBN13') or extract_field(x, 'ISBN 13'))\n",
    "            df['publisher'] = df['details'].apply(lambda x: extract_field(x, 'Publisher'))\n",
    "            df = df.drop(columns=['details'])\n",
    "\n",
    "        # Step 3: Filter for Books category\n",
    "        print(\"Filtering for Books category...\")\n",
    "        if 'main_category' in df.columns:\n",
    "            books_mask = df['main_category'].fillna('').astype(str).eq('Books')\n",
    "            df_books = df[books_mask].copy()\n",
    "            print(f\"After Books filtering: {df_books.shape}\")\n",
    "            if df_books.empty:\n",
    "                print(\"No books found in this chunk, creating empty json file\")\n",
    "                result = []\n",
    "                print(f\"Saving to {output_file_path}...\")\n",
    "                with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "                return True\n",
    "        else:\n",
    "            print(\"main_category column not found, skipping filtering and keeping original dataframe.\")\n",
    "            df_books = df.copy()\n",
    "\n",
    "        # Step 4: Select and rename columns\n",
    "        print(\"Selecting final columns...\")\n",
    "        keep_columns = [\n",
    "            'title', 'subtitle',\n",
    "            'author_name', 'author_avatar', 'author_about',\n",
    "            'categories',\n",
    "            'publisher', 'ISBN 10', 'ISBN 13', 'main_image_url'\n",
    "        ]\n",
    "        existing_columns = [col for col in keep_columns if col in df_books.columns]\n",
    "        df_books = df_books[existing_columns].copy()\n",
    "\n",
    "        # Step 5: Drop rows with missing values in required fields\n",
    "        print(\"Dropping rows with missing values...\")\n",
    "        required_fields = ['title', 'author_name', 'ISBN 10', 'ISBN 13']\n",
    "        existing_required = [f for f in required_fields if f in df_books.columns]\n",
    "        if existing_required:\n",
    "            df_books = df_books.dropna(subset=existing_required).copy()\n",
    "            print(f\"After dropping missing values: {df_books.shape}\")\n",
    "        else:\n",
    "            print(\"No required fields found in dataframe, skipping dropna\")\n",
    "\n",
    "        # Step 6: Restore structure\n",
    "        print(\"Restoring final structure...\")\n",
    "        result = df_books.to_dict(orient='records')\n",
    "\n",
    "        # Save the results\n",
    "        print(f\"Saving to {output_file_path}...\")\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(f\"Successfully processed {chunk_file}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in chunk {chunk_file}:\")\n",
    "        print(traceback.format_exc())\n",
    "        return False\n",
    "\n",
    "# Main processing loop\n",
    "chunk_files = [f for f in os.listdir(input_dir) if f.endswith('.jsonl')]\n",
    "if not chunk_files:\n",
    "    print(f\"No JSONL files found in {input_dir}\")\n",
    "else:\n",
    "    for chunk_file in chunk_files:\n",
    "        safe_process_chunk(chunk_file)\n",
    "\n",
    "print(\"Processing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6509568-30dc-4ffd-ac3e-e3ab183961ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
